{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "import spacy\n",
    "from operator import itemgetter\n",
    "import numpy as np\n",
    "import io\n",
    "import random\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import time\n",
    "import torch.nn.functional as F\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA is False\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "#Check if cuda is available\n",
    "cuda = torch.cuda.is_available()\n",
    "print('CUDA is', cuda)\n",
    "\n",
    "num_workers = 8 if cuda else 0\n",
    "print(num_workers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with io.open('../Data/glove.6B.50d.txt', 'r', encoding='utf8') as f:\n",
    "    glove_file = f.read()\n",
    "    \n",
    "glove_sentences = glove_file.splitlines()\n",
    "glove_vocab = {}\n",
    "for sentence in glove_sentences:\n",
    "    word = sentence.split()[0]\n",
    "    embedding = np.array(sentence.split()[1:], dtype = float)\n",
    "    glove_vocab[word] = embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open('../Data/furniture_cleaned-tagged_m.json',) \n",
    "data = json.load(f)\n",
    "\n",
    "#CALCULATING AMBIGUITY SCORES IN IS ADJECTIVES\n",
    "ambiguity_m = {}\n",
    "for element in data[-1]:\n",
    "    if element[3] == 'a':\n",
    "        score = 0\n",
    "    else:\n",
    "        score = 1\n",
    "    ambiguity_m[element[0]] = [element[2], score]\n",
    "    \n",
    "    \n",
    "f = open('../Data/furniture_cleaned-tagged_a.json',) \n",
    "data = json.load(f)\n",
    "\n",
    "#CALCULATING AMBIGUITY SCORES IN IS ADJECTIVES\n",
    "ambiguity_a = {}\n",
    "for element in data[-1]:\n",
    "    if element[3] == 'a':\n",
    "        score = 0\n",
    "    else:\n",
    "        score = 1\n",
    "    ambiguity_a[element[0]] = [element[2], score]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "#GIVING PREFERENCE TO AKSHAT'S LABELS. REVERSE THE ORDER TO GIVE PREFERENCE TO MANUEL'S LABELS\n",
    "ambiguity = {}\n",
    "\n",
    "for adj in ambiguity_m:\n",
    "    if int(ambiguity_m[adj][0]) !=0 and adj in glove_vocab:\n",
    "        ambiguity[adj] = ambiguity_m[adj][1]\n",
    "        \n",
    "for adj in ambiguity_a:\n",
    "    if int(ambiguity_a[adj][0]) !=0 and adj not in ambiguity and adj in glove_vocab:\n",
    "        ambiguity[adj] = ambiguity_a[adj][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data = []\n",
    "for adj in ambiguity:\n",
    "    all_data.append([glove_vocab[adj], ambiguity[adj]])\n",
    "    \n",
    "random.shuffle(all_data)\n",
    "size = len(all_data)\n",
    "training_data = all_data[:int(size*0.9)]\n",
    "test_data = all_data[int(size*0.9):]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDataset(Dataset):\n",
    "    def __init__(self, X):\n",
    "        self.X = X\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self,index):\n",
    "\n",
    "        return torch.from_numpy(self.X[index][0]).float(), self.X[index][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size=8\n",
    "train_dataset = MyDataset(training_data)\n",
    "train_loader = DataLoader(train_dataset, shuffle = True, batch_size = batch_size)\n",
    "\n",
    "test_dataset = MyDataset(test_data)\n",
    "test_loader = DataLoader(test_dataset, shuffle = False, batch_size = batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "class My_MLP_Model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(My_MLP_Model, self).__init__()\n",
    "        #self.batchnorm1 = nn.BatchNorm1d(50)\n",
    "        self.fc1 = nn.Linear(50, 128)\n",
    "        \n",
    "        #self.batchnorm2 = nn.BatchNorm1d(64)\n",
    "        self.fc2 = nn.Linear(128, 8)\n",
    "        \n",
    "        #self.batchnorm3 = nn.BatchNorm1d(32)\n",
    "        #self.fc3 = nn.Linear(32, 8)\n",
    "        \n",
    "        #self.batchnorm_last = nn.BatchNorm1d(8)\n",
    "        self.fc_last = nn.Linear(8, 2)\n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        #x = self.batchnorm1(x)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        \n",
    "        #x = self.batchnorm2(x)\n",
    "        x = F.relu(self.fc2(x))\n",
    "        \n",
    "        #x = self.batchnorm3(x)\n",
    "        #x = F.relu(self.fc3(x))\n",
    "        \n",
    "        #x = self.batchnorm_last(x)\n",
    "        x = self.fc_last(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, train_loader, criterion, optimizer):\n",
    "    model.train()\n",
    "\n",
    "    running_loss = 0.0\n",
    "    \n",
    "    start_time = time.time()\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):   \n",
    "        optimizer.zero_grad()   # .backward() accumulates gradients\n",
    "        data = data.to(device)\n",
    "        target = target.to(device) # all data & model on same device\n",
    "\n",
    "        outputs = model(data)\n",
    "        loss = criterion(outputs, target)\n",
    "        running_loss += loss.item()\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    end_time = time.time()\n",
    "    \n",
    "    running_loss /= len(train_loader)\n",
    "    print('Training Loss: ', running_loss, 'Time: ',end_time - start_time, 's')  \n",
    "    return running_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_model(model, validate_loader, criterion):\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "\n",
    "        running_loss = 0.0\n",
    "        total_predictions = 0.0\n",
    "        correct_predictions = 0.0\n",
    "\n",
    "        for batch_idx, (data, target) in enumerate(validate_loader):   \n",
    "            data = data.to(device)\n",
    "            target = target.to(device)\n",
    "\n",
    "            outputs = model(data)\n",
    "\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total_predictions += target.size(0)\n",
    "            correct_predictions += (predicted == target).sum().item()\n",
    "\n",
    "            loss = criterion(outputs, target).detach()\n",
    "            running_loss += loss.item()\n",
    "\n",
    "\n",
    "        running_loss /= len(validate_loader)\n",
    "        acc = (correct_predictions/total_predictions)*100.0\n",
    "        print('Testing Loss: ', running_loss)\n",
    "        print('Testing Accuracy: ', acc, '%')\n",
    "        return running_loss, acc\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss:  0.5852306027934976 Time:  0.1419057846069336 s\n",
      "Testing Loss:  0.4314960075749291\n",
      "Testing Accuracy:  73.84615384615385 %\n",
      "====================\n",
      "Training Loss:  0.39284284025022426 Time:  0.13092708587646484 s\n",
      "Testing Loss:  0.3190989171465238\n",
      "Testing Accuracy:  87.6923076923077 %\n",
      "====================\n",
      "Training Loss:  0.3384446555825129 Time:  0.13776493072509766 s\n",
      "Testing Loss:  0.30942145196927917\n",
      "Testing Accuracy:  87.6923076923077 %\n",
      "====================\n",
      "Training Loss:  0.31461253496882036 Time:  0.13621306419372559 s\n",
      "Testing Loss:  0.31826478698187405\n",
      "Testing Accuracy:  87.6923076923077 %\n",
      "====================\n",
      "Training Loss:  0.2926799265591249 Time:  0.18563604354858398 s\n",
      "Testing Loss:  0.31519974561201203\n",
      "Testing Accuracy:  86.15384615384616 %\n",
      "====================\n",
      "Training Loss:  0.26986031805815763 Time:  0.12368416786193848 s\n",
      "Testing Loss:  0.3350101417551438\n",
      "Testing Accuracy:  86.15384615384616 %\n",
      "====================\n",
      "Training Loss:  0.24658011849204156 Time:  0.21042895317077637 s\n",
      "Testing Loss:  0.32923365839653546\n",
      "Testing Accuracy:  84.61538461538461 %\n",
      "====================\n",
      "Training Loss:  0.22163416371259786 Time:  0.13657402992248535 s\n",
      "Testing Loss:  0.34905367924107444\n",
      "Testing Accuracy:  83.07692307692308 %\n",
      "====================\n",
      "Training Loss:  0.1954387351105066 Time:  0.12676525115966797 s\n",
      "Testing Loss:  0.3571065595994393\n",
      "Testing Accuracy:  84.61538461538461 %\n",
      "====================\n",
      "Training Loss:  0.1673604790121317 Time:  0.12479209899902344 s\n",
      "Testing Loss:  0.3722083872805039\n",
      "Testing Accuracy:  83.07692307692308 %\n",
      "====================\n"
     ]
    }
   ],
   "source": [
    "model = My_MLP_Model()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr = 0.001)\n",
    "#optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n",
    "device = torch.device(\"cuda\" if cuda else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "for i in range(10):\n",
    "    train_loss = train_epoch(model, train_loader, criterion, optimizer)\n",
    "    test_loss, test_acc = validate_model(model, test_loader, criterion)\n",
    "\n",
    "    print('='*20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
