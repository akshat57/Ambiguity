{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "import spacy\n",
    "from operator import itemgetter\n",
    "import numpy as np\n",
    "import io\n",
    "import random\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import time\n",
    "import torch.nn.functional as F\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA is False\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "#Check if cuda is available\n",
    "cuda = torch.cuda.is_available()\n",
    "print('CUDA is', cuda)\n",
    "\n",
    "num_workers = 8 if cuda else 0\n",
    "print(num_workers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with io.open('../Data/glove.6B.50d.txt', 'r', encoding='utf8') as f:\n",
    "    glove_file = f.read()\n",
    "    \n",
    "glove_sentences = glove_file.splitlines()\n",
    "glove_vocab = {}\n",
    "for sentence in glove_sentences:\n",
    "    word = sentence.split()[0]\n",
    "    embedding = np.array(sentence.split()[1:], dtype = float)\n",
    "    glove_vocab[word] = embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open('../Data/furniture_cleaned-tagged_m.json',) \n",
    "data = json.load(f)\n",
    "\n",
    "#CALCULATING AMBIGUITY SCORES IN IS ADJECTIVES\n",
    "ambiguity_m = {}\n",
    "for element in data[-1]:\n",
    "    if element[3] == 'a':\n",
    "        score = 0\n",
    "    else:\n",
    "        score = 1\n",
    "    ambiguity_m[element[0]] = [element[2], score]\n",
    "    \n",
    "    \n",
    "f = open('../Data/furniture_cleaned-tagged_a.json',) \n",
    "data = json.load(f)\n",
    "\n",
    "#CALCULATING AMBIGUITY SCORES IN IS ADJECTIVES\n",
    "ambiguity_a = {}\n",
    "for element in data[-1]:\n",
    "    if element[3] == 'a':\n",
    "        score = 0\n",
    "    else:\n",
    "        score = 1\n",
    "    ambiguity_a[element[0]] = [element[2], score]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#GIVING PREFERENCE TO Manuel'S LABELS. REVERSE THE ORDER TO GIVE PREFERENCE TO Akshat'S LABELS\n",
    "ambiguity = {}\n",
    "\n",
    "for adj in ambiguity_m:\n",
    "    if int(ambiguity_m[adj][0]) !=0 and adj in glove_vocab:\n",
    "        ambiguity[adj] = ambiguity_m[adj][1]\n",
    "        \n",
    "for adj in ambiguity_a:\n",
    "    if int(ambiguity_a[adj][0]) !=0 and adj not in ambiguity and adj in glove_vocab:\n",
    "        ambiguity[adj] = ambiguity_a[adj][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data = []\n",
    "for adj in ambiguity:\n",
    "    all_data.append([glove_vocab[adj], ambiguity[adj]])\n",
    "    \n",
    "random.shuffle(all_data)\n",
    "size = len(all_data)\n",
    "#training_data = all_data[:int(size*0.9)]\n",
    "#test_data = all_data[int(size*0.9):]\n",
    "\n",
    "#creating a balanced test set\n",
    "split = 0.1 #for test set\n",
    "n_test = int((size * split)/2)\n",
    "counters = [0, 0]\n",
    "training_data = []\n",
    "test_data = []\n",
    "\n",
    "for word, label in all_data:\n",
    "    if counters[label] < n_test:\n",
    "        test_data.append([word, label])\n",
    "        counters[label] += 1\n",
    "    else:\n",
    "        training_data.append([word, label])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDataset(Dataset):\n",
    "    def __init__(self, X):\n",
    "        self.X = X\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self,index):\n",
    "\n",
    "        return torch.from_numpy(self.X[index][0]).float(), self.X[index][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size=8\n",
    "train_dataset = MyDataset(training_data)\n",
    "train_loader = DataLoader(train_dataset, shuffle = True, batch_size = batch_size)\n",
    "\n",
    "test_dataset = MyDataset(test_data)\n",
    "test_loader = DataLoader(test_dataset, shuffle = False, batch_size = batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "class My_MLP_Model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(My_MLP_Model, self).__init__()\n",
    "        #self.batchnorm1 = nn.BatchNorm1d(50)\n",
    "        self.fc1 = nn.Linear(50, 128)\n",
    "        \n",
    "        #self.batchnorm2 = nn.BatchNorm1d(64)\n",
    "        self.fc2 = nn.Linear(128, 8)\n",
    "        \n",
    "        #self.batchnorm3 = nn.BatchNorm1d(32)\n",
    "        #self.fc3 = nn.Linear(32, 8)\n",
    "        \n",
    "        #self.batchnorm_last = nn.BatchNorm1d(8)\n",
    "        self.fc_last = nn.Linear(8, 2)\n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        #x = self.batchnorm1(x)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        \n",
    "        #x = self.batchnorm2(x)\n",
    "        x = F.relu(self.fc2(x))\n",
    "        \n",
    "        #x = self.batchnorm3(x)\n",
    "        #x = F.relu(self.fc3(x))\n",
    "        \n",
    "        #x = self.batchnorm_last(x)\n",
    "        x = self.fc_last(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, train_loader, criterion, optimizer):\n",
    "    model.train()\n",
    "\n",
    "    running_loss = 0.0\n",
    "    total_predictions = 0.0\n",
    "    correct_predictions = 0.0\n",
    "    \n",
    "    predictions = []\n",
    "    ground_truth = []\n",
    "    \n",
    "    start_time = time.time()\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):   \n",
    "        optimizer.zero_grad()   # .backward() accumulates gradients\n",
    "        data = data.to(device)\n",
    "        target = target.to(device) # all data & model on same device\n",
    "\n",
    "        outputs = model(data)\n",
    "        loss = criterion(outputs, target)\n",
    "        running_loss += loss.item()\n",
    "        \n",
    "        #calculating accuracy\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total_predictions += target.size(0)\n",
    "        correct_predictions += (predicted == target).sum().item()\n",
    "            \n",
    "        #calculuating confusion matrix\n",
    "        predictions += list(predicted.numpy())\n",
    "        ground_truth += list(target.numpy())\n",
    "        \n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    end_time = time.time()\n",
    "    \n",
    "    print('------ Training -----')\n",
    "    print('')\n",
    "    #print('Confusion Matrix')\n",
    "    #print(confusion_matrix(ground_truth, predictions))\n",
    "    #print('F1 scores')\n",
    "    #print(classification_report(ground_truth, predictions))\n",
    "    running_loss /= len(train_loader)\n",
    "    acc = (correct_predictions/total_predictions)*100.0\n",
    "    print('Training Loss: ', running_loss, 'Time: ',end_time - start_time, 's')\n",
    "    print('Training Accuracy: ', acc, '%')\n",
    "    return running_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_model(model, validate_loader, criterion):\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "\n",
    "        running_loss = 0.0\n",
    "        total_predictions = 0.0\n",
    "        correct_predictions = 0.0\n",
    "        \n",
    "        predictions = []\n",
    "        ground_truth = []\n",
    "\n",
    "        for batch_idx, (data, target) in enumerate(validate_loader):   \n",
    "            data = data.to(device)\n",
    "            target = target.to(device)\n",
    "\n",
    "            outputs = model(data)\n",
    "\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total_predictions += target.size(0)\n",
    "            correct_predictions += (predicted == target).sum().item()\n",
    "            \n",
    "            #calculuating confusion matrix\n",
    "            predictions += list(predicted.numpy())\n",
    "            ground_truth += list(target.numpy())\n",
    "\n",
    "            loss = criterion(outputs, target).detach()\n",
    "            running_loss += loss.item()\n",
    "\n",
    "        print('------ Testing -----')\n",
    "        print('')\n",
    "        print('Confusion Matrix')\n",
    "        print(confusion_matrix(ground_truth, predictions))\n",
    "        print('F1 scores')\n",
    "        print(classification_report(ground_truth, predictions))\n",
    "        running_loss /= len(validate_loader)\n",
    "        acc = (correct_predictions/total_predictions)*100.0\n",
    "        print('Testing Loss: ', running_loss)\n",
    "        print('Testing Accuracy: ', acc, '%')\n",
    "        return running_loss, acc\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------ Training -----\n",
      "\n",
      "Training Loss:  0.47735679231277883 Time:  0.11297202110290527 s\n",
      "Training Accuracy:  79.7598627787307 %\n",
      "------ Testing -----\n",
      "\n",
      "Confusion Matrix\n",
      "[[ 0 32]\n",
      " [ 0 32]]\n",
      "F1 scores\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00        32\n",
      "           1       0.50      1.00      0.67        32\n",
      "\n",
      "    accuracy                           0.50        64\n",
      "   macro avg       0.25      0.50      0.33        64\n",
      "weighted avg       0.25      0.50      0.33        64\n",
      "\n",
      "Testing Loss:  0.7997210882604122\n",
      "Testing Accuracy:  50.0 %\n",
      "====================\n",
      "------ Training -----\n",
      "\n",
      "Training Loss:  0.35423441517026455 Time:  0.0849299430847168 s\n",
      "Training Accuracy:  79.7598627787307 %\n",
      "------ Testing -----\n",
      "\n",
      "Confusion Matrix\n",
      "[[ 0 32]\n",
      " [ 0 32]]\n",
      "F1 scores\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00        32\n",
      "           1       0.50      1.00      0.67        32\n",
      "\n",
      "    accuracy                           0.50        64\n",
      "   macro avg       0.25      0.50      0.33        64\n",
      "weighted avg       0.25      0.50      0.33        64\n",
      "\n",
      "Testing Loss:  0.8492535538971424\n",
      "Testing Accuracy:  50.0 %\n",
      "====================\n",
      "------ Training -----\n",
      "\n",
      "Training Loss:  0.3272535733572424 Time:  0.08312797546386719 s\n",
      "Training Accuracy:  79.7598627787307 %\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------ Testing -----\n",
      "\n",
      "Confusion Matrix\n",
      "[[ 0 32]\n",
      " [ 0 32]]\n",
      "F1 scores\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00        32\n",
      "           1       0.50      1.00      0.67        32\n",
      "\n",
      "    accuracy                           0.50        64\n",
      "   macro avg       0.25      0.50      0.33        64\n",
      "weighted avg       0.25      0.50      0.33        64\n",
      "\n",
      "Testing Loss:  0.8326409794390202\n",
      "Testing Accuracy:  50.0 %\n",
      "====================\n",
      "------ Training -----\n",
      "\n",
      "Training Loss:  0.30977777366156445 Time:  0.09466385841369629 s\n",
      "Training Accuracy:  79.7598627787307 %\n",
      "------ Testing -----\n",
      "\n",
      "Confusion Matrix\n",
      "[[ 0 32]\n",
      " [ 0 32]]\n",
      "F1 scores\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00        32\n",
      "           1       0.50      1.00      0.67        32\n",
      "\n",
      "    accuracy                           0.50        64\n",
      "   macro avg       0.25      0.50      0.33        64\n",
      "weighted avg       0.25      0.50      0.33        64\n",
      "\n",
      "Testing Loss:  0.8712504021823406\n",
      "Testing Accuracy:  50.0 %\n",
      "====================\n",
      "------ Training -----\n",
      "\n",
      "Training Loss:  0.284255924672909 Time:  0.0840451717376709 s\n",
      "Training Accuracy:  83.87650085763293 %\n",
      "------ Testing -----\n",
      "\n",
      "Confusion Matrix\n",
      "[[17 15]\n",
      " [ 4 28]]\n",
      "F1 scores\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      0.53      0.64        32\n",
      "           1       0.65      0.88      0.75        32\n",
      "\n",
      "    accuracy                           0.70        64\n",
      "   macro avg       0.73      0.70      0.69        64\n",
      "weighted avg       0.73      0.70      0.69        64\n",
      "\n",
      "Testing Loss:  0.9050834588706493\n",
      "Testing Accuracy:  70.3125 %\n",
      "====================\n",
      "------ Training -----\n",
      "\n",
      "Training Loss:  0.25189382041970343 Time:  0.10782074928283691 s\n",
      "Training Accuracy:  89.87993138936535 %\n",
      "------ Testing -----\n",
      "\n",
      "Confusion Matrix\n",
      "[[18 14]\n",
      " [ 5 27]]\n",
      "F1 scores\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.78      0.56      0.65        32\n",
      "           1       0.66      0.84      0.74        32\n",
      "\n",
      "    accuracy                           0.70        64\n",
      "   macro avg       0.72      0.70      0.70        64\n",
      "weighted avg       0.72      0.70      0.70        64\n",
      "\n",
      "Testing Loss:  0.8565801568329334\n",
      "Testing Accuracy:  70.3125 %\n",
      "====================\n",
      "------ Training -----\n",
      "\n",
      "Training Loss:  0.22365417770326954 Time:  0.10466599464416504 s\n",
      "Training Accuracy:  91.59519725557462 %\n",
      "------ Testing -----\n",
      "\n",
      "Confusion Matrix\n",
      "[[18 14]\n",
      " [ 5 27]]\n",
      "F1 scores\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.78      0.56      0.65        32\n",
      "           1       0.66      0.84      0.74        32\n",
      "\n",
      "    accuracy                           0.70        64\n",
      "   macro avg       0.72      0.70      0.70        64\n",
      "weighted avg       0.72      0.70      0.70        64\n",
      "\n",
      "Testing Loss:  0.9728074129670858\n",
      "Testing Accuracy:  70.3125 %\n",
      "====================\n",
      "------ Training -----\n",
      "\n",
      "Training Loss:  0.195943043586377 Time:  0.08630609512329102 s\n",
      "Training Accuracy:  93.65351629502572 %\n",
      "------ Testing -----\n",
      "\n",
      "Confusion Matrix\n",
      "[[18 14]\n",
      " [ 5 27]]\n",
      "F1 scores\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.78      0.56      0.65        32\n",
      "           1       0.66      0.84      0.74        32\n",
      "\n",
      "    accuracy                           0.70        64\n",
      "   macro avg       0.72      0.70      0.70        64\n",
      "weighted avg       0.72      0.70      0.70        64\n",
      "\n",
      "Testing Loss:  0.9756424650549889\n",
      "Testing Accuracy:  70.3125 %\n",
      "====================\n",
      "------ Training -----\n",
      "\n",
      "Training Loss:  0.1717420792436763 Time:  0.2183079719543457 s\n",
      "Training Accuracy:  94.16809605488851 %\n",
      "------ Testing -----\n",
      "\n",
      "Confusion Matrix\n",
      "[[19 13]\n",
      " [ 5 27]]\n",
      "F1 scores\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.79      0.59      0.68        32\n",
      "           1       0.68      0.84      0.75        32\n",
      "\n",
      "    accuracy                           0.72        64\n",
      "   macro avg       0.73      0.72      0.71        64\n",
      "weighted avg       0.73      0.72      0.71        64\n",
      "\n",
      "Testing Loss:  1.1000122837722301\n",
      "Testing Accuracy:  71.875 %\n",
      "====================\n",
      "------ Training -----\n",
      "\n",
      "Training Loss:  0.1504045634001034 Time:  0.0881490707397461 s\n",
      "Training Accuracy:  95.19725557461408 %\n",
      "------ Testing -----\n",
      "\n",
      "Confusion Matrix\n",
      "[[18 14]\n",
      " [ 5 27]]\n",
      "F1 scores\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.78      0.56      0.65        32\n",
      "           1       0.66      0.84      0.74        32\n",
      "\n",
      "    accuracy                           0.70        64\n",
      "   macro avg       0.72      0.70      0.70        64\n",
      "weighted avg       0.72      0.70      0.70        64\n",
      "\n",
      "Testing Loss:  1.1039675511419773\n",
      "Testing Accuracy:  70.3125 %\n",
      "====================\n"
     ]
    }
   ],
   "source": [
    "model = My_MLP_Model()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr = 0.001)\n",
    "#optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n",
    "device = torch.device(\"cuda\" if cuda else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "for i in range(10):\n",
    "    train_loss = train_epoch(model, train_loader, criterion, optimizer)\n",
    "    test_loss, test_acc = validate_model(model, test_loader, criterion)\n",
    "\n",
    "    print('='*20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ConfusionMatrixDisplay', 'PrecisionRecallDisplay', 'RocCurveDisplay', 'SCORERS', '__all__', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__path__', '__spec__', '_base', '_classification', '_pairwise_fast', '_plot', '_ranking', '_regression', '_scorer', 'accuracy_score', 'adjusted_mutual_info_score', 'adjusted_rand_score', 'auc', 'average_precision_score', 'balanced_accuracy_score', 'brier_score_loss', 'calinski_harabasz_score', 'calinski_harabaz_score', 'check_scoring', 'classification_report', 'cluster', 'cohen_kappa_score', 'completeness_score', 'confusion_matrix', 'consensus_score', 'coverage_error', 'davies_bouldin_score', 'dcg_score', 'euclidean_distances', 'explained_variance_score', 'f1_score', 'fbeta_score', 'fowlkes_mallows_score', 'get_scorer', 'hamming_loss', 'hinge_loss', 'homogeneity_completeness_v_measure', 'homogeneity_score', 'jaccard_score', 'jaccard_similarity_score', 'label_ranking_average_precision_score', 'label_ranking_loss', 'log_loss', 'make_scorer', 'matthews_corrcoef', 'max_error', 'mean_absolute_error', 'mean_gamma_deviance', 'mean_poisson_deviance', 'mean_squared_error', 'mean_squared_log_error', 'mean_tweedie_deviance', 'median_absolute_error', 'multilabel_confusion_matrix', 'mutual_info_score', 'nan_euclidean_distances', 'ndcg_score', 'normalized_mutual_info_score', 'pairwise', 'pairwise_distances', 'pairwise_distances_argmin', 'pairwise_distances_argmin_min', 'pairwise_distances_chunked', 'pairwise_kernels', 'plot_confusion_matrix', 'plot_precision_recall_curve', 'plot_roc_curve', 'precision_recall_curve', 'precision_recall_fscore_support', 'precision_score', 'r2_score', 'recall_score', 'roc_auc_score', 'roc_curve', 'silhouette_samples', 'silhouette_score', 'v_measure_score', 'zero_one_loss']\n"
     ]
    }
   ],
   "source": [
    "print(dir(sklearn.metrics))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function classification_report in module sklearn.metrics._classification:\n",
      "\n",
      "classification_report(y_true, y_pred, labels=None, target_names=None, sample_weight=None, digits=2, output_dict=False, zero_division='warn')\n",
      "    Build a text report showing the main classification metrics\n",
      "    \n",
      "    Read more in the :ref:`User Guide <classification_report>`.\n",
      "    \n",
      "    Parameters\n",
      "    ----------\n",
      "    y_true : 1d array-like, or label indicator array / sparse matrix\n",
      "        Ground truth (correct) target values.\n",
      "    \n",
      "    y_pred : 1d array-like, or label indicator array / sparse matrix\n",
      "        Estimated targets as returned by a classifier.\n",
      "    \n",
      "    labels : array, shape = [n_labels]\n",
      "        Optional list of label indices to include in the report.\n",
      "    \n",
      "    target_names : list of strings\n",
      "        Optional display names matching the labels (same order).\n",
      "    \n",
      "    sample_weight : array-like of shape (n_samples,), default=None\n",
      "        Sample weights.\n",
      "    \n",
      "    digits : int\n",
      "        Number of digits for formatting output floating point values.\n",
      "        When ``output_dict`` is ``True``, this will be ignored and the\n",
      "        returned values will not be rounded.\n",
      "    \n",
      "    output_dict : bool (default = False)\n",
      "        If True, return output as dict\n",
      "    \n",
      "    zero_division : \"warn\", 0 or 1, default=\"warn\"\n",
      "        Sets the value to return when there is a zero division. If set to\n",
      "        \"warn\", this acts as 0, but warnings are also raised.\n",
      "    \n",
      "    Returns\n",
      "    -------\n",
      "    report : string / dict\n",
      "        Text summary of the precision, recall, F1 score for each class.\n",
      "        Dictionary returned if output_dict is True. Dictionary has the\n",
      "        following structure::\n",
      "    \n",
      "            {'label 1': {'precision':0.5,\n",
      "                         'recall':1.0,\n",
      "                         'f1-score':0.67,\n",
      "                         'support':1},\n",
      "             'label 2': { ... },\n",
      "              ...\n",
      "            }\n",
      "    \n",
      "        The reported averages include macro average (averaging the unweighted\n",
      "        mean per label), weighted average (averaging the support-weighted mean\n",
      "        per label), and sample average (only for multilabel classification).\n",
      "        Micro average (averaging the total true positives, false negatives and\n",
      "        false positives) is only shown for multi-label or multi-class\n",
      "        with a subset of classes, because it corresponds to accuracy otherwise.\n",
      "        See also :func:`precision_recall_fscore_support` for more details\n",
      "        on averages.\n",
      "    \n",
      "        Note that in binary classification, recall of the positive class\n",
      "        is also known as \"sensitivity\"; recall of the negative class is\n",
      "        \"specificity\".\n",
      "    \n",
      "    See also\n",
      "    --------\n",
      "    precision_recall_fscore_support, confusion_matrix,\n",
      "    multilabel_confusion_matrix\n",
      "    \n",
      "    Examples\n",
      "    --------\n",
      "    >>> from sklearn.metrics import classification_report\n",
      "    >>> y_true = [0, 1, 2, 2, 2]\n",
      "    >>> y_pred = [0, 0, 2, 2, 1]\n",
      "    >>> target_names = ['class 0', 'class 1', 'class 2']\n",
      "    >>> print(classification_report(y_true, y_pred, target_names=target_names))\n",
      "                  precision    recall  f1-score   support\n",
      "    <BLANKLINE>\n",
      "         class 0       0.50      1.00      0.67         1\n",
      "         class 1       0.00      0.00      0.00         1\n",
      "         class 2       1.00      0.67      0.80         3\n",
      "    <BLANKLINE>\n",
      "        accuracy                           0.60         5\n",
      "       macro avg       0.50      0.56      0.49         5\n",
      "    weighted avg       0.70      0.60      0.61         5\n",
      "    <BLANKLINE>\n",
      "    >>> y_pred = [1, 1, 0]\n",
      "    >>> y_true = [1, 1, 1]\n",
      "    >>> print(classification_report(y_true, y_pred, labels=[1, 2, 3]))\n",
      "                  precision    recall  f1-score   support\n",
      "    <BLANKLINE>\n",
      "               1       1.00      0.67      0.80         3\n",
      "               2       0.00      0.00      0.00         0\n",
      "               3       0.00      0.00      0.00         0\n",
      "    <BLANKLINE>\n",
      "       micro avg       1.00      0.67      0.80         3\n",
      "       macro avg       0.33      0.22      0.27         3\n",
      "    weighted avg       1.00      0.67      0.80         3\n",
      "    <BLANKLINE>\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(help(sklearn.metrics.classification_report))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
